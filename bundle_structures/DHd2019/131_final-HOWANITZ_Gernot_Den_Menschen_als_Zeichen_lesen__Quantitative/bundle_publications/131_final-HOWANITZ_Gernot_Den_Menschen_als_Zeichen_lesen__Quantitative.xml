<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:id="131_final-HOWANITZ_Gernot_Den_Menschen_als_Zeichen_lesen__Quantitative" xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">Den Menschen als Zeichen lesen. Quantitative Lesarten körperlicher Zeichenhaftigkeit in visuellen Medien</title>
                    <title type="sub"/>
                </title>
                <author>
                    <persName>
                        <surname>Howanitz</surname>
                        <forename>Gernot</forename>
                    </persName>
                    <affiliation>Universität Passau, Deutschland</affiliation>
                    <email>gernot.howanitz@uni-passau.de</email>
                </author>
                <author>
                    <persName>
                        <surname>Radisch</surname>
                        <forename>Erik</forename>
                    </persName>
                    <affiliation>Universität Passau, Deutschland</affiliation>
                    <email>erik.radisch@uni-passau.de</email>
                </author>
                <author>
                    <persName>
                        <surname>Decker</surname>
                        <forename>Jan-Oliver</forename>
                    </persName>
                    <affiliation>Universität Passau, Deutschland</affiliation>
                    <email>Jan-Oliver.Decker@Uni-Passau.De</email>
                </author>
                <author>
                    <persName>
                        <surname>Rehbein</surname>
                        <forename>Malte</forename>
                    </persName>
                    <affiliation>Universität Passau, Deutschland</affiliation>
                    <email>malte.rehbein@uni-passau.de</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2019-01-13T15:45:10.713954739</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <t:publisher xmlns:t="http://www.tei-c.org/ns/1.0">Patrick Sahle, im Auftrag des Verbands Digital Humanities im deutschsprachigen Raum e.V.</t:publisher>
                <t:address xmlns:t="http://www.tei-c.org/ns/1.0">
                    <t:addrLine>Universität zu Köln</t:addrLine>
                    <t:addrLine>Cologne Center for eHumanities</t:addrLine>
                    <t:addrLine>Albertus-Magnus-Platz</t:addrLine>
                    <t:addrLine>50923 Köln</t:addrLine>
                </t:address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Vortrag</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Distant Watching</term>
                    <term>Bilder</term>
                    <term>Videos</term>
                    <term>Menschen</term>
                    <term>Visual Computing</term>
                    <term>Posenerkennung</term>
                    <term>Mimikerkennung</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Bilderfassung</term>
                    <term>Inhaltsanalyse</term>
                    <term>Kontextsetzung</term>
                    <term>Bilder</term>
                    <term>Personen</term>
                    <term>Video</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Trotz der Textlastigkeit der Digital Humanities ist in der letzten Zeit ein gewisser beginnender „visualistic turn“ in der Disziplin zu beobachten, der in den Kulturwissenschaften bereits seit einiger Zeit konstatiert wird. (Sachs-Hombach 1993) Auch der Schwerpunkt dieser Konferenz weist in diese Richtung. Visuelle Medien bieten zweifelsohne eine große Chance für eine Weiterentwicklung der Digital Humanities, trotzdem ist festzustellen, dass sich dieser Turn bisher noch größtenteils auf die Kunstgeschichte 
            konzentriert.<ref target="ftn1" n="1"/> Das liegt aufgrund des visuellen Schwerpunktes dieser Disziplin zwar nahe, jedoch widmen sich auch andere Disziplinen, in unserem Falle die Kulturwissenschaften, aus der Perspektive ihrer spezifischen Fragestellungen den visuellen Medien.
            </p>
            <p>Das Anliegen dieses Vortrages ist es, einen Zugang zu präsentieren, der es Kulturwissenschaftler/inne/n ermöglicht, große Bild- und Videokorpora in einem Methodenmix sowohl qualitativer als auch quantitativer Verfahren zu erfassen. Auf der letzten DHd-Konferenz haben wir erste Vorarbeiten in Form eines Ansatzes präsentiert, der aus visuellen Medien den relevanten symbolischen Kontext identifiziert und damit eine automatisierte Informationsextraktion und -analyse ermöglicht (Bermeitinger/Howanitz/Radisch 2018). Ein großes Videokorpus konnte auf diese Weise bearbeitet werden, ohne alle Videos im einzelnen ansehen zu müssen. Das Auftreten von Menschen in diesen Videos oder Bildern wurde dabei lediglich am Rande geschnitten, indem die Möglichkeit in Erwägung gezogen wurde, bestimmte Personen auf Einzelframes zu identifizieren.</p>
            <p>Hier möchte unser diesjähriger Vortrag ansetzen und Jurij Lotmans (1984) semiotischen Ansätzen folgend menschliche (Ab-)Bilder zeichenhaft in visuellen Korpora lesen. Als Beispielkorpus dient uns eine Sammlung von Starpostkarten von Marlene Dietrich aus den 1930er und 1940er Jahren. Ziel ist es, ähnlich wie bei Distant Reading-Ansätzen zu versuchen, versteckte Muster, Gemeinsamkeiten und Abweichungen ohne Betrachtung der Einzelbilder identifizieren zu können. Dabei erweist sich das Erscheinen von Menschen in diesen Bildern als zentral und muss besonders berücksichtigt werden. Um menschliche Abbildungen in visuellen Korpora möglichst umfassend analysieren zu können, schlagen wir eine dreistufige Herangehensweise vor, nämlich die Identifikation einzelner Personen, die Analyse von Körperhaltungen, sowie die Mimikerfassung. Im Folgenden werden alle drei Teilbereiche vorgestellt.</p>
            <div type="div1">
                <head>
                    <anchor xml:id="id__qj4rf9ao527u"/>Identifikation
                </head>
                <p>In vielen Fällen der Analyse menschlicher Zeichenhaftigkeit ist es von großem Vorteil, bestimmte Personen identifizieren zu können. So ist es zum Beispiel interessant, die Auswahl an Personen in einem visuellen Korpus durch Kriterien der Relevanz in bezug auf eine konkrete Forschungsfrage einzuschränken. Auch können auf diese Weise verschiedene Protagonisten und in weiterer Folge deren Kookkurrenzen innerhalb des Korpus identifiziert werden, was Rückschlüsse auf Personenkonstellationen zulässt. Handelt es sich bei den analysierten Bildern um Einzelframes aus Videos, erlaubt es dieser Ansatz zudem, resultierende Muster der Personenkonstellationen als Netzwerke zu visualisieren – analog zu der von Franco Moretti etablierten Netzwerkanalyse von Dramen, die in den letzten Jahren zahlreiche Anwendung gefunden hat.</p>
                <p>Zum Erkennen von Gesichtern verwenden wir David Sandbergs 
                    <hi rend="italic">Facenet</hi>, das auf OpenFace (Amos/Ludwiczuk/Satyanarayanan 2016) beruht und folgen dabei den Empfehlungen unseres Kollegen Sebastian Gassner (Gassner 2018). Hinzuweisen ist aber hier ebenfalls auf das Distant-Viewing Toolkit von Lauren Tilton und Taylor Arnold, das ebenfalls ermöglicht, bestimmte Akteure in Filmen identifizieren zu können (Tilton/Arnold 2018).
                </p>
            </div>
            <div type="div1">
                <head>
                    <anchor xml:id="id__95cxb5pq6go8"/>Haltung
                </head>
                <p>Mit der bloßen Anwesenheit bestimmter Personen in visuellen Medien ist aber noch kaum Aussagen über deren Zeichenhaftigkeit möglich. Vielmehr gilt es auch die Körperhaltung der Personen einer eingehenden Analyse zu unterziehen. Gerade hier manifestiert sich die Zeichenhaftigkeit eines Körpers besonders. Der gesamte Körper fungiert als ein Zeichen. Je nach Haltung können völlig unterschiedliche Aussagen getätigt werden, die nur zum Teil einerseits bewusst intendiert sind, andererseits bewusst wahrgenommen werden. Hier erlauben es algorithmische Verfahren, einen Blick auf das Phänomen zeichenhafter Körperlichkeit zu ermöglichen, der etwas weniger durch menschliche Erfahrung beeinflusst ist als dies bei qualitativen Ansätzen der Fall ist.</p>
                <p>Körperhaltungen lassen sich auf Grundlage einiger wichtiger Keypoints (Bourdev/Malik 2009) analysieren, die die Position der Hände, Ellenbogen, Schultern, der Hüfte, der Knie und der Füße sowie des Kopfes angeben. Je nachdem, wie diese Positionen in Relation zueinander stehen, ergeben sich verschiedene Haltungen die Rückschlüsse auf symbolische, über den Körper kommunizierte Bedeutungen erlauben. Solche Ansätze wurden erst kürzlich von Leonardo Impetti und Franco Moretti zur Klassifizierung von Aby Warburgs Bilderatlas verfolgt (Impett/Moretti 2017). Ähnliche Ansätze werden auch in unserem Projekt herangezogen. So erlaubt es beispielsweise die Clusterung nach den Keypoints in unserem Korpus bestimmte Körperhaltungen zusammenzufassen und deren Entwicklung diachron über die Zeitspanne des Korpus zu untersuchen. Der Zusammenhang zwischen Körperhaltungen und Genderstereotypen ist vieldiskutiert (für einen körpersemiotischen Zugang vgl. Mühlen-Achs 1998); gerade im Zusammenhang mit Marlene Dietrichs androgyner Selbstinszenierung, die männlich konnotierte Elemente aufgreift, ermöglicht eine quantitative Analyse dieser Körperhaltungen diesbezüglich neue Einblicke. Daneben interessieren uns auch Kopfhaltung und insbesondere die Blickrichtung, die ihrerseits die Grundparameter der zugrunde liegenden kommunikativen Situation offenbart.</p>
                <p>
                    <figure>
                        <graphic url="131_final-3e7ad32d6c9e9ffab33c2cf41bbaf835.png"/>
                        <head>
                            <lb/><hi rend="bold">Abb. 1</hi>: 
                            <hi rend="italic">Detectron </hi>identifiziert Marlene Dietrichs typische Doppelamphorenhaltung (links), rechts eine Figurenkonstellation.
                        </head>
                    </figure>
                </p>
                <p>Der Ansatz der Posen-Erkennung hat noch ein wesentlich höheres Potential für zukünftige Forschung. Hierüber könnten sich auch Bewegungsabläufe in Videos analysieren lassen, da sich bestimmte Bedeutungsebenen erst im Kontext des Zusammenspiels ergeben.</p>
                <p>Technisch greifen wir einerseits auf die Keypoint-Ebene von 
                    <hi rend="italic">Detectron </hi>(Girshik et al. 2018)
                    <hi rend="italic"> </hi>zurück, eines Frameworks für Deep Learning und Objekterkennung, zur Verfügung gestellt von 
                    <hi rend="italic">Facebook Artificial Intelligence Research</hi>, andererseits auf das von Cao et al. (2017) veröffentlichte System zur Posen-Erkennung, das im Vergleich zu 
                    <hi rend="italic">Detectron </hi>weitere Keypoints identifiziert und darüber hinaus zwischen den beiden Körperhälften unterscheiden kann.
                </p>
                <p>Ein Nachteil der hier verwendeten Algorithmen ist, dass sie die Keypoints nur zweidimensional erkennen, also die Position auf dem Bild erfassen, nicht aber deren Lage im Raum. Für eine dreidimensionale Erfassung zumindest des Gesichts wird mit 
                    <hi rend="italic">OpenFace </hi>(Baltrušaitis et al. 2018) deshalb noch ein dritter Algorithmus verwendet, der eine Abschätzung der dreidimensionalen Lage – insbesondere auch der Kameraposition in Relation zum Gesicht – und der Blickrichtung erlaubt. Mithilfe dieser Informationen können grobe Rückschlüsse auf die Gesamtkomposition einer Szene gemacht werden.
                </p>
                <p>
                    <figure>
                        <graphic url="131_final-7f660cfa59f31160794e04fbf7e21ea9.png"/>
                        <head>
                            <lb/><hi rend="bold">Abb. 2</hi>: OpenFace blickt Marlene Dietrich ins Gesicht: Lage im Raum (blau), Keypoints (rot), berechnete Blickrichtung (grün)
                        </head>
                    </figure>
                </p>
            </div>
            <div type="div1">
                <head>
                    <anchor xml:id="id__bz819zs56wfs"/>Mimik
                </head>
                <p>
                    <hi rend="italic">OpenFace </hi>erlaubt es des Weiteren eine weitere Ebene menschlicher Zeichenhaftigkeit in visuellen Medien zu erschließen – das Gesicht. Dabei wird neben der dreidimensionalen Position des Kopfes ebenfalls alle wichtigen Punkte innerhalb des Gesichtes, darunter die Umrisse von Augen, Nase, Lippen, Augenbrauen und Kinn erkannt (Abb. 2). Der Algorithmus ermittelt die Abweichung dieser Gesichtspunkte von einem Standardmodell, welche wiederum mithilfe des Facial Action Coding System (FACS), ein System der Taxonomie menschlicher Gesichtsausdrücke, beschrieben werden können (Ekman/Friesen 1978). FACS besteht aus einer Reihe von Action Units, die basale Gesichtsbewegungen klassifizieren, wie beispielsweise die Hebung der inneren Augenbrauen (AU1), die Senkung der Augenbrauen (AU4), das Zusammenkneifen der Lider (AU7) und das Anziehen der Mundwinkel (AU12).
                </p>
                <p>Diese AUs stellen einen Standard zur Beschreibung von Gesichtsausdrücken dar. </p>
                <p>Damit stellt OpenFace ein ideales Werkzeug dar, um Mimiken in visuellen Medien auslesen zu können, mit einer großen potenziellen Breite an Einsatzmöglichkeiten. So lassen sich beispielsweise damit Bilder mit Personen nach Gesichtsausdrücken clustern oder die Häufigkeit bestimmter Action Units im Zeitverlauf zu analysieren, um so Rückschlüsse über veränderte kulturelle Praxen machen zu können. Im Falle von Marlene Dietrich ist beispielsweise beobachtbar, dass AU2 (Hebung der äußeren Augenbrauen) eine Zeit lang häufig auftritt, was auf die Schminkgewohnheiten der damaligen Zeit zurückzuführen ist. Auf diese Weise können auch Modetrends und Stilfragen bis zu einem gewissen Grad quantitativ erfasst werden.</p>
                <p>Solche Informationen können aber auch in anderen Kontexten von großen analytischen Wert sein. In einem Korpus von 290 Zigarettenbildern zeigen sich etwa klare Unterschiede zwischen den Geschlechtern. AU6 (Wangen heben) kombiniert mit AU12 (Mundwinkel heben) entspricht etwa der Emotion „Freude“, die bei den Frauen häufiger anzutreffen ist. AU15 (Mundwinkel senken) ist hingegen bei den Männern weitaus verbreiteter (siehe Abb. 3).</p>
                <p>
                    <figure>
                        <graphic url="131_final-fb4a5f85aac6664beeaec4483d64bd4e.png"/>
                        <head>
                            <lb/><hi rend="bold">Abb. 3</hi>: Histogramme von AU6, AU12 und AU15 bei Frauen (grün) und Männern (orange).
                        </head>
                    </figure>
                </p>
            </div>
            <div type="div1">
                <head>
                    <anchor xml:id="id__our1iawf6o2i"/>Das Puzzle setzt sich zusammen: Quantitative Lesarten körperlicher Zeichenhaftigkeit
                </head>
                <p>Die oben beschriebene Methodenkombination ist einerseits nötig, um körperliche Zeichenhaftigkeit in visuellen Medien auf den verschiedenen skizzierten Bedeutungsebenen erfassen zu können, erschwert aber andererseits das Arbeiten mit dem Korpus und verlangt deshalb nach einer entsprechenden methodischen Aufbereitung. Das erste Ziel unseres Beitrages ist deshalb, die Methoden durch ein Testkorpus von rund 200 Bildern systematisch zu evaluieren. Anschließend wenden wir die Methoden auf ein Korpus von mehreren tausend deutschen Starpostkarten vorwiegend aus den 1930ern und 1940ern an, die wir in Zusammenarbeit mit dem Filmmuseum Potsdam digitalisiert haben. Als Basis unserer Arbeiten dient ein eigenständiges Framework, das wir für mehrere Forschungsprojekte innerhalb des vom BMBF geförderten Passau Centre for eHumanities 
                (PACE, Fördernummer: <hi rend="color(#00000a)">01UG1602</hi>) entwickelt haben und die es ermöglichen verschiedene quantitative Aspekte mit qualitativen Fragestellungen kurzzuschließen. Wie bereits erwähnt, ist es ein 
                    <hi rend="italic">desideratum,</hi> das System auch für Bewegtbilder zu öffnen und weiterzuentwickeln, auch wenn der damit verbundene Aufwand nicht trivial ist.
                </p>
            </div>
        </body>
        <back>
            <div type="notes">
                <note xml:id="1" n="1" rend="footnote text">
                    Der Schwerpunkt auf Kunstgeschichte spiegelt sich beispielsweise im DFG-Schwerpunktprogramm “Das digitale Bild” wieder, dass zwar prinzipiell auch für die Kulturwissenschaften geöffnet war, aber in der Ausschreibung einen deutlichen Fokus auf die Kunstgeschichte nahm.
                </note>
            </div>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl>
                        <hi rend="bold">Amos, Brandon / Ludwiczuk, Bartosz / Satyanarayanan, Mahadev (2016)</hi>: <hi rend="italic">"Openface: A general-purpose face recognition library with mobile applications,"</hi> CMU-CS-16-118, CMU School of Computer Science, Tech. Rep., 2016. <ptr target="https://www.cs.cmu.edu/~satya/docdir/CMU-CS-16-118.pdf"/> [letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Arnold, Taylor / Tilton, Lauren (2018)</hi>: <hi rend="italic">„Distant Viewing Toolkit (DVT) for the Cultural Analysis of Moving Images“</hi>, https://github.com/distant-viewing/dvt [letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Baltrušaitis, Tadas / Zadeh, Amir / Chong Lim, Yao / Morency, Louis-Philippe (2018)</hi>: <hi rend="italic">“OpenFace 2.0: Facial Behavior Analysis Toolkit“</hi>, IEEE International Conference on Automatic Face and Gesture Recognition 2018., <ptr target="https://ieeexplore.ieee.org/document/8373812"/> [letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Bermeitinger, Bernhard / Howanitz, Gernot / Radisch, Erik (2018)</hi>: <hi rend="italic">“Contextualising Bandera. Eine Distant Watching-Methode”</hi>, DHd 2018.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Bourdev, Lubomir / Malik, Jitendra (2009)</hi>: <hi rend="italic">“Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations”</hi>, ICCV 2009. <ptr target="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/human/poselets_iccv09.pdf"/> [letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Ekman, Paul / Friesen, Wallace V. (1978)</hi>: <hi rend="italic">The Facial Action Coding System: A Technique for the Measurement of Facial Movement</hi>. Palo Alto: Consulting.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Gassner, Sebastian (2018)</hi>: <hi rend="italic">„How to Use David Sandberg's Facenet Implementation“</hi>, <ref target="https://github.com/sepastian/facenet/blob/master/HOWTO.md">https://github.com/sepastian/facenet/blob/master/HOWTO.md</ref> [letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Girshick, Ross / Radosavovic, Ilija / Gkioxari, Georgia / Dollár, Piotr / He, Kaiming (2018)</hi>: <hi rend="italic">Detectron</hi>. <ptr target="https://github.com/facebookresearch/detectron"/>[letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Cao, Zhe / Simon, Tomas / Wei, Shih-En / Sheikh, Yaser (2017)</hi>: <hi rend="italic">„Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields“</hi>. CVPR 2017. <ptr target="https://arxiv.org/pdf/1611.08050.pdf"/>[letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>I
                        <hi rend="bold">mpett, Leonardo / Moretti, Franco (2017)</hi>: <hi rend="italic">“Totentanz. Operationalizing Aby Warburg’s Pathosformeln.”</hi> In: Literary Lab Pamphlet 16, November 2017, <ptr target="https://litlab.stanford.edu/LiteraryLabPamphlet16.pdf"/> [letzter Zugriff 29. 9. 2018].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Lotman, Jurij (1984)</hi>: <hi rend="italic">“O semiosfere”</hi>. Učen. zap. Tart. gos. un-ta 641 (1984), 5-23. (=Trudy po znakovym sistemam 17).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Mühlen-Achs, Gitta (1998)</hi>: <hi rend="italic">Geschlecht bewusst gemacht. Körpersprachliche Inszenierungen</hi>. München: Frauenoffensive.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Sachs-Hombach, Klaus (1993)</hi>: <hi rend="italic">Das Bild als kommunikatives Medium. Elemente einer allgemeinen Bildwissenschaft. </hi>Köln: Halem.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
