<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:id="144_final-SCHNEIDER_Stefanie__ber_die_Ungleichheit_im_Gleichen__Erkenn" xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">Über die Ungleichheit im Gleichen. Erkennung unterschiedlicher Reproduktionen desselben Objekts in kunsthistorischen Bildbeständen</title>
                    <title type="sub"/>
                </title>
                <author>
                    <persName>
                        <surname>Schneider</surname>
                        <forename>Stefanie</forename>
                    </persName>
                    <affiliation>Ludwig-Maximilians-Universität München, Deutschland</affiliation>
                    <email>stefanie.schneider@itg.uni-muenchen.de</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2016-08-22T21:51:20.48</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <t:publisher xmlns:t="http://www.tei-c.org/ns/1.0">Patrick Sahle, im Auftrag des Verbands Digital Humanities im deutschsprachigen Raum e.V.</t:publisher>
                <t:address xmlns:t="http://www.tei-c.org/ns/1.0">
                    <t:addrLine>Universität zu Köln</t:addrLine>
                    <t:addrLine>Cologne Center for eHumanities</t:addrLine>
                    <t:addrLine>Albertus-Magnus-Platz</t:addrLine>
                    <t:addrLine>50923 Köln</t:addrLine>
                </t:address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Vortrag</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Reproduktion</term>
                    <term>Objektidentifikation</term>
                    <term>Kunstgeschichte</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Beziehungsanalyse</term>
                    <term>Bereinigung</term>
                    <term>Archivierung</term>
                    <term>Identifizierung</term>
                    <term>Artefakte</term>
                    <term>Bilder</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Es finden sich mittlerweile mannigfaltige Studien, die Objekte historischen Ursprungs in den Fokus rücken und zwischen ihnen bestehende Relationen und Ähnlichkeitsverhältnisse mathematisch abzubilden versuchen (darunter Bergel et al., 2013; Monroy et al., 2014; Hentschel et al., 2016). Eine fundamentale Herausforderung 
                <hi rend="italic">per se</hi> heterogener historischer Inventare bleibt dabei unberührt: digitale Reproduktionen ein und desselben Objekts, sogenannte 
                <hi rend="italic">Near-duplicates</hi> oder 
                <hi rend="italic">Near-replicas</hi>, die als separate Einträge in Aggregatordatenbanken vorgehalten werden und sich bspw. hinsichtlich ihres Farbstichs oder ihrer Helligkeit unterscheiden. Aufgrund nicht-standardisierter, teils unstrukturierter oder selten kontrollierten Vokabularen zugeordneter Metadaten ist es zumeist nicht oder nur mit größtem Aufwand möglich, derartige „Kopien“ auf Basis textueller Information sowohl nachhaltig als auch zuverlässig ohne händische Nacharbeit zu verknüpfen.
            </p>
            <p>Unser Ansatz zielt auf dreierlei: erstens die automatische Zusammenführung unterschiedlicher Reproduktionen desselben Objekts; die zweitens das 
                <hi rend="italic">Retrieval</hi> variierender Reproduktionen erlaubt, um weiterführende Analysen, z. B. quellenkritischer Art, über eben jenes Objekt anstoßen zu können; und drittens die Extraktion textueller Information von Objekten, die ausschließlich visuell, d. h. als digitales Bild, vorliegen und, im Sinne einer 
                <hi rend="italic">Reverse Image Search</hi>, mit einem bereits annotierten Inventar von Objekten abzugleichen sind. Auf diese Weise wird nicht nur eine effiziente, bildbasierte Suche in Datenbanken insbesondere (aber nicht ausschließlich) kunsthistorischer Objekte ermöglicht, sondern nahezu 
                <hi rend="italic">Bias</hi>-freie statistische Untersuchungen, wie sie durch den Einfluss häufig reproduzierter Werke bislang nicht gegeben 
                waren.<ref target="ftn1" n="1"/>
            </p>
            <div type="div1" rend="DH-Heading1">
                <head>Methode</head>
                <p>Wir stützen uns hauptsächlich auf 
                    <hi rend="italic">Scale Invariant Feature Transform</hi> 
                    (<hi rend="italic">SIFT</hi>; Lowe, 1999; Lowe, 2004), um aus Bildern historischer Objekte lokale Schlüsselpunkte 
                    (<hi rend="italic">Keypoints</hi>) zu extrahieren und mittels Deskriptoren 
                    (<hi rend="italic">Descriptors</hi>) weiterverarbeiten zu können. Schlüsselpunkte bilden einzelne Interessenregionen eines Bildes ab, die statistisch, aber nicht notwendigerweise semantisch relevante Merkmale tragen, und stellen sie in einem 128-dimensionalen Histogramm dar. Im mathematischen Sinne sind sie Extremwerte, die über einen Raum mehrfach skalierter und mit einem Gaußfilter geglätteter Bilder ermittelt werden. Mit üblichen Ähnlichkeits- und Distanzmaßen, z. B. der euklidischen Distanz, ist es so möglich, die Nähe zwischen zwei Schlüsselpunkten zu quantifizieren, und demnach auch, über die Summe der 
                    <hi rend="italic">matchenden</hi> Schlüsselpunkte, die Nähe zwischen zwei Digitalisaten; wobei anzunehmen ist, dass die Anzahl der übereinstimmenden Schlüsselpunkte für variierende Abbildungen desselben Objekts höher ist als für Abbildungen unterschiedlicher Objekte.
                </p>
                <p>Ein Bild wird je nach Größe und Detailgrad mit Hunderten bis Tausenden derartiger Schlüsselpunkte assoziiert. Daraus resultierende computationale Kosten fangen wir durch drei Erweiterungen des Verfahrens ab. Erstens reduzieren wir die Dimensionalität der Deskriptoren mittels 
                    <hi rend="italic">Principal Component Analysis</hi> 
                    (<hi rend="italic">PCA</hi>). Im Gegensatz zu Ke und Sukthankar (2004) greifen wir nicht in den Deskriptionsprozess selbst ein, sondern ermitteln den Eigenraum auf Basis der standardmäßig durch 
                    <hi rend="italic">SIFT</hi> eruierten Deskriptoren. Zweitens verringern wir die Anzahl der Schlüsselpunkte, indem wir zunächst die mit dem höchsten Kontrast auswählen und auf Basis dessen jene filtern, welche die größten Interessenregionen charakterisieren. Damit werden auf flächenmäßig kleinen Arealen zu findende, kontrastreiche Schlüsselpunkte getilgt, die bspw. in textuellen Ergänzungen von Kupferstichen auftreten und für das 
                    <hi rend="italic">Matching</hi> irrelevant bis schädlich sind. Drittens setzen wir mit 
                    <hi rend="italic">Hierarchical Navigable Small World</hi> 
                    (<hi rend="italic">HNSW</hi>; Malkov und Yashunin, 2016) einen 
                    <hi rend="italic">Approximate Nearest Neighbor-</hi>Ansatz mit polylogarithmischer Komplexität ein, der in aktuellen repräsentativen Benchmarks andere Graph-basierte Ansätze in Präzision und Schnelligkeit übertrifft (Aumueller et al., 2018). Eine adaptive Intensitätskorrektur jedes Bildes durch 
                    <hi rend="italic">Contrast Limited Adaptive Histogram Equalization</hi> 
                    (<hi rend="italic">CLAHE</hi>; Zuiderweld, 1994) wird vor der Extraktion der Schlüsselpunkte durchgeführt, um stark über- oder unterbelichtete Reproduktionen anzupassen.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Daten</head>
                <p>Ein geeigneter 
                    <hi rend="italic">Gold Standard</hi> wird in drei Schritten etabliert. Zunächst ziehen wir eine Zufallsstichprobe von 3.581 kunsthistorischen Objekten, die in der Datenbank 
                    <hi rend="italic">ArteMIS</hi> des Instituts für Kunstgeschichte der Ludwig-Maximilians-Universität München verzeichnet 
                    sind<ref target="ftn2" n="2"/> und einen angemessenen Querschnitt verschiedener kunsthistorischer Stile und Epochen erlauben; Holzschnitte sind ebenso inkludiert wie realistische Landschaftsmalerei und Werke des französischen 
                    Impressionismus.<ref target="ftn3" n="3"/> In einem zweiten Schritt speisen wir Titel und Künstler jener Objekte in die 94 Datenbanken kumulierende 
                    <hi rend="italic">Application Programming Interface</hi> von 
                    Prometheus.<ref target="ftn4" n="4"/> Da in den jeweiligen Suchergebnissen nicht nur Digitalisate ein und desselben Objekts zu finden sind – Vorzeichnungen und aufgrund ihrer Metadaten ähnliche Reproduktionen sind auch darunter –, schließen wir einen dritten Schritt an, in dem auf unterschiedliche Objekte referenzierende Abbildungen manuell entfernt werden. Es verbleiben 9.934 Reproduktionen. Ein derart selegiertes Digitalisat trägt einen eineindeutigen Identifikator, der sowohl auf das es abbildende Objekt zeigt als auch auf die an das Digitalisat gekoppelten Metadaten weist.
                </p>
                <p>Um weitere in der bildarchivarischen Praxis existente, aber durch zuvor extrahierte 
                    <hi rend="italic">reale</hi> Digitalisate unzureichend abgedeckte Modifikationen, bspw. größere Änderungen des Kontrasts oder der Sättigung eines Bildes, untersuchen zu können, generieren wir 278.152 zusätzliche, sogenannte 
                    <hi rend="italic">synthetische</hi> Kopien. Jede Ursprungsreproduktion wird dementsprechend dupliziert und 28 mathematischen Transformationen unterzogen; ähnlich zu jenen in Ke et al. (2004), Qamra et al. (2005) und Foo et al. (2007). Unter anderem modelliert werden sich in ihrer Stärke unterscheidende nicht-lineare Verzerrungen, die Wölbungen nahe des Buchrückens von Fotografien kunsthistorischer Publikationen suggerieren.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Evaluation</head>
                <p>Drei im 
                    <hi rend="italic">Information Retrieval</hi> gewöhnliche Gütekriterien dienen der Evaluation der angewandten Methoden: Precision, Recall und 
                    F<hi rend="sub">1</hi>-Maß. Wir gehen wie folgt vor. Der Satz an Objekten, die mit realen und synthetischen Reproduktionen assoziiert sind, wird unterteilt in 25 zufällig separierte Trainings- und Teststichproben, wobei jeweils 80 Prozent der Objekte der Trainings- und 20 Prozent der Teststichprobe zuzuordnen sind. Auf Basis der 25-fachen Kreuzvalidierung erhalten wir durchschnittliche Werte für Precision, Recall und 
                    F<hi rend="sub">1</hi>-Maß, die von einem jeweils gegebenen Schwellenwert abhängen, der die minimale Anzahl der Schlüsselpunkte bezeichnet, die zwischen zwei Digitalisaten übereinstimmen müssen, damit diese als unterschiedliche Reproduktionen desselben Objekts gelten und entsprechend zusammengeführt werden können. Der jeweils für eine Parameterkonstellation optimale Schwellenwert bildet sich aus dem Modus der 25 Einzelschwellenwerte, die mit dem höchsten 
                    F<hi rend="sub">1</hi>-Maß verknüpft sind.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Ergebnisse</head>
                <p>Wir stellen fest, dass aufgrund des hohen Anteils in den Reproduktionen enthaltener digitaler Artefakte – unter anderem Bildrauschen und Unschärfe –, Konfigurationen mit im Vergleich zu Standardwerten höherem Skalenparameter σ, der die Stärke des in 
                    <hi rend="italic">SIFT</hi> angelegten gaußschen Weichzeichners reguliert, und niedrigeren Schwellenwerten, welche die Aufnahme von kontrastarmen oder auf Kanten situierten Schlüsselpunkten steuern, zu bevorzugen sind. Eine Reduktion der Dimensionalität der Deskriptoren und der Anzahl der Schlüsselpunkte auf jeweils 50 führt zu marginalen Einbußen in Precision und Recall, steigert jedoch maßgeblich die Performanz und mindert den notwendigen Speicherbedarf. Eine so klassifizierte, aus 500 Objekten bestehende Zufallsstichprobe resultiert in Precision = 0,9857, Recall = 0,9820 und 
                    F<hi rend="sub">1</hi>-Maß = 0,9839, wenn für 
                    <hi rend="italic">HNSW</hi> moderate Kompromisse zwischen der Geschwindigkeit, die der Aufbau des Index und die eine Suche im Index benötigt, formuliert werden; mindestens 7 näherungsweise übereinstimmende Schlüsselpunkte sind erforderlich, damit Digitalisate als demselben Objekt zugehörig erkannt werden. Größere Einbrüche in Recall, d. h. bis zu 5 Prozentpunkte, sind für stärkere Farbänderungen und nicht-lineare Verzerrungen zu beobachten. Insbesondere drei Gruppen von Objekten erfordern weitere Anpassungen: Digitalisate von Druckgrafiken mit hohen Kontrastunterschieden; Reproduktionen, die Rahmen oder rahmenähnliche Strukturen abbilden; Werke des Impressionismus und nicht-gegenständliche oder diffuse Werke, die unterdurchschnittlich viele Schlüsselpunkte, teilweise nur bis zu 10, produzieren.
                </p>
                <p>Auch ohne zusätzliche Modifikationen zeigt sich, dass die hier präsentierte Methode hinreichend exakte Ergebnisse erwarten lässt und kaum, oder lediglich im Falle hoch spezialisierter Korpora, manuelle Adjustierungen erfordert; selbst wenn stärkere Abweichungen in Kontrast oder Sättigung auftreten. Durch die Integration eines 
                    <hi rend="italic">Approximate Nearest Neighbor-</hi>Ansatzes ist weiterhin gewährleistet, dass das Verfahren auch auf größere historische Bildbestände skaliert.
                </p>
            </div>
        </body>
        <back>
            <div type="notes">
                <note xml:id="ftn1" n="1" rend="footnote text">
                    Dies schließt anderweitige Verzerrungen, bspw. einen <hi rend="italic">Selection Bias</hi>, natürlich nicht aus.
                </note>
                <note xml:id="ftn2" n="2" rend="footnote text">
                    Eine Online-Schnittstelle ist zu erreichen unter <ptr target="http://artemis.lmu.de/"/> (25.09.2018).
                </note>
                <note xml:id="ftn3" n="3" rend="footnote text">
                    Ausgenommen werden Reproduktionen eindeutig dreidimensionaler Objekte, z. B. Plastiken und Skulpturen, da sich diese zusätzlich durch den bei der Aufnahme eingenommenen Blickwinkel unterscheiden können und folglich gesondert zu evaluieren wären.
                </note>
                <note xml:id="ftn4" n="4" rend="footnote text">
                    <ptr target="http://www.prometheus-bildarchiv.de/"/> (25.09.2018).
                </note>
            </div>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl>
						<hi rend="bold">Aumueller, Martin / Bernhardsson, Erik / Faitfull, Alec (2018)</hi>: <hi rend="italic">ANN Benchmarks</hi>, <ptr target="http://sss.projects.itu.dk/ann-benchmarks/index.html"/> (26.09.2018).
                    </bibl>
                    <bibl>
						<hi rend="bold">Bergel, Giles / Franklin, Alexandra / Heaney, Michael / Arand-Jelovic, Relja / Zisserman, Andrew / Funke, Donata (2013)</hi>: <hi rend="italic">„Content-based Image Recognition on Printed Broadside Ballads. The Bodleian Libraries’ Imagematch Tool“</hi>, in: Proceedings of the IFLA World Library and Information Congress.
                    </bibl>
                    <bibl>
						<hi rend="bold">Foo, Jun Jie / Zobel, Justin / Sinha, Ranjan (2007)</hi>: <hi rend="italic">„Clustering Near-duplicate Images in Large Collections“</hi>, in: Proceedings of the ACM SIGMM International Workshop on Multimedia Information Retrieval 21–30.
                    </bibl>
                    <bibl>
						<hi rend="bold">Hentschel, Christian / Wiradarma, Timur P. / Sack, Harald (2016)</hi>: <hi rend="italic">„An Approach to Large Scale Interactive Retrieval of Cultural Heritage“</hi>, in: Proceedings of the 23th IEEE International Conference on Image Processing 3693–3697.
                    </bibl>
                    <bibl>
						<hi rend="bold">Ke, Yan / Sukthankar, Rahul (2004)</hi>: <hi rend="italic">„PCA-SIFT. A More Distinctive Representation for Local Image Descriptors“</hi>, in: Proceedings of the IEEE International Conference on Computer and Pattern Recognition 506–513.
                    </bibl>
                    <bibl>
						<hi rend="bold">Ke, Yan / Sukthankar, Rahul / Huston, Larry (2004)</hi>: <hi rend="italic">„An Efficient Parts-based Near-duplicate and Sub-image Retrieval System“</hi>, in: Proceedings of the >12th ACM International Conference on Multimedia 869–876.
                    </bibl>
                    <bibl>
						<hi rend="bold">Lowe, David G. (1999)</hi>: <hi rend="italic">„Object Recognition from Local Scale-invariant Features“</hi>, in: Proceedings of the 7th IEEE International Conference on Computer Vision 1150–1157.
                    </bibl>
                    <bibl>
						<hi rend="bold">Lowe, David G. (2004)</hi>: <hi rend="italic">„Distinctive Image Features from Scale-invariant Keypoints“</hi>, in: International Journal of Computer Vision 60 (2): 91–110.
                    </bibl>
                    <bibl>
						<hi rend="bold">Malkov, Yury A. / Yashunin, Dmitry A. (2016)</hi>: <hi rend="italic">Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs</hi>.
                    </bibl>
                    <bibl>
						<hi rend="bold">Monroy, Antonio / Bell, Peter / Ommer, Björn (2014)</hi>: <hi rend="italic">„Morphological Analysis for Investigating Artistic Images“</hi>, in: Image and Vision Computing 32 (6): 414–423.
                    </bibl>
                    <bibl>
						<hi rend="bold">Qamra, Arun / Meng, Yan / Chang, Edward Y. (2005)</hi>: <hi rend="italic">„Enhanced Perceptual Distance Functions and Indexing for Image Replica Recognition“</hi>, in: IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (3): 379–391.
                    </bibl>
                    <bibl>
						<hi rend="bold">Zuiderveld, Karel (1994)</hi>: <hi rend="italic">„Contrast Limited Adaptive Histogram Equalization“</hi>, in: Academic Press 474–485.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
