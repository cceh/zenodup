<?xml version="1.0" encoding="utf-8"?>
<TEI xml:id="168_final-GIUS_Evelyn_Computationelle_Textanalyse_als_f_nfdimensionale" xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Computationelle Textanalyse als fünfdimensionales Problem</title>
<author>
<persName>
<surname>Gius</surname>
<forename>Evelyn</forename>
</persName>
<affiliation>Technische Universität Darmstadt, Deutschland</affiliation>
<email>gius@linglit.tu-darmstadt.de</email>
</author>
</titleStmt>
<editionStmt>
<edition>
<date>2019-12-19T14:40:00Z</date>
</edition>
</editionStmt>
<publicationStmt>
<publisher>Universität Paderborn</publisher>
<address>
<addrLine>Warburger Str. 100</addrLine>
<addrLine>33098 Paderborn</addrLine>
<addrLine>Deutschland</addrLine>
</address>
</publicationStmt>
<sourceDesc>
<p>Converted from a Word document</p>
</sourceDesc>
</fileDesc>
<encodingDesc>
<appInfo>
<application ident="DHCONVALIDATOR" version="1.22">
<label>DHConvalidator</label>
</application>
</appInfo>
</encodingDesc>
<profileDesc>
<textClass>
<keywords n="category" scheme="ConfTool">
<term>Paper</term>
</keywords>
<keywords n="subcategory" scheme="ConfTool">
<term>Vortrag</term>
</keywords>
<keywords n="keywords" scheme="ConfTool">
<term>Komplexität</term>
<term>Phänomene</term>
<term>Erkenntnisinteresse</term>
</keywords>
<keywords n="topics" scheme="ConfTool">
<term>Modellierung</term>
<term>Theoretisierung</term>
<term>Bewertung</term>
<term>Text</term>
</keywords>
</textClass>
</profileDesc>
</teiHeader>
<text>
<body>
<div rend="DH-Heading1" type="div1">
<head>Die Einschätzung und Kritik computationeller Textanalyse</head>
<p style="text-align:left; ">In diesem Beitrag wird ein Modell vorgestellt, das zu einer Einschätzung der Komplexität von Forschungsansätzen dient, die sich Texten mit computationellen Analysen nähern. Das Modell wurde vor dem Hintergrund der (literaturwissenschaftlichen) Analyse von literarischen Texten entwickelt, es ist jedoch – ggf. mit leichten Anpassungen – für Textanalysen generell geeignet.</p>
<p style="text-align:left; ">Die Komplexität von Digital Humanities-Projekten ist bestimmt von der Aushandlung von Vorannahmen, Methoden, der Passung zum Gegenstand, der konkreten interdisziplinären Zusammenarbeit, die fachlich, persönlich und oft auch karrierestrategisch eine große Herausforderung für die Beteiligten sein kann, bis hin zur Darstellung von Ergebnissen für eine oder mehrere Forschungscommunities. Neben Fragen der Projektplanung und -steuerung, wissenschaftspolitischen und wissenschaftskommunikativen Aspekten geht es auch um Fragen, die das eigentliche Forschungsgeschehen betreffen. Dieses wird aktuell in Bezug auf seine Relevanz und Ausrichtung diskutiert: Eine harsche Kritik von Nan Z. Da (2019a) an den Verfahren der DH initiierte eine mit dem etwas überzogenen Begriff „Digital Humanities War“ bezeichnete Auseinandersetzung.<ref n="1" target="ftn1"/> Diese Debatte wird z.T. als Auseinandersetzung zwischen angeblichen Strukturalist*innen und Poststrukturalist*innen dargestellt. Zumindest von letzteren, die den Strukturalismus als solchen benennen und eine Kluft zwischen diesem und den eigenen Zugängen diagnostizieren (vgl. z.B. Dobson 2019 und Bode im Erscheinen). Hinzu kommt, dass in der Auseinandersetzung förderpolitische Aspekte zumindest als Hintergrund eine große Rolle spielen.<ref n="2" target="ftn2"/>
</p>
</div>
<div rend="DH-Heading1" type="div1">
<head>Ein methodenunabhängiges <lb/>Modell</head>
<p style="text-align:left; ">Diese Auseinandersetzungen gehen zum Großteil an den eigentlichen Forschungszugängen vorbei. Dabei wäre es aus Sicht der Digital Humanities 
                    <hi rend="italic">und</hi> der Literaturwissenschaft erhellend, die diskutierten Verfahren oder gar Methodenlinien detaillierter zu beschreiben und ihre Bedeutung zu reflektieren. Deshalb möchte ich ein Modell vorschlagen, das eine solche Betrachtung von computationellen Textanalyseansätzen ermöglicht und eine Grundlage bildet, auf der Textanalyse-Zugänge unabhängig von ihrer literaturtheoretischen Fundierung beschrieben, kritisiert und zu verglichen werden können.
                </p>
<p style="text-align:left; ">Ausgangspunkt des Modells sind die drei Aspekte, die für jede computergestützte Textanalyse wesentlich sind: Die Phänomene, denen das Interesse gilt, die Texte, die untersucht werden, und die Art, wie Erkenntnis erzeugt wird.<ref n="3" target="ftn3"/> Aus diesen Aspekten lassen sich insgesamt fünf Dimensionen ableiten, die für die Einschätzung der Komplexität eines Zugangs genutzt werden können: die Kontextualisierung von Phänomenen, die Zusammengesetztheit von Phänomenen, die Heterogenität von Texten, der Analysemodus und der Erkenntnisbeitrag computationeller Analysen. 
                </p>
<div rend="DH-Heading2" type="div2">
<head>Zusammengesetztheit von Phänomenen</head>
<p style="text-align:left; ">Eine Einschätzung der Phänomene, die in einer computationellen Textanalyse untersucht werden, kann anhand der Phänomenbeschreibung stattfinden. Für diese kann man fragen: Wird das Phänomen als einfach, nicht weiter unterteilt, oder als aus mehreren Phänomenen zusammengesetzt betrachtet? Dabei geht es wohlgemerkt nicht um eine allgemein gültige Definition des entsprechenden Phänomens, sondern um die von den Forscher*innen genutzte Beschreibung.</p>
<p style="text-align:left; ">Beschreibungen für dasselbe Phänomen können in unterschiedlichen Forschungsprojekten entsprechend unterschiedlich ausfallen. In Bezug auf ein aktuelles Forschungsprojekt zu Gender und Krankheit in literarischen Prosatexten<ref n="4" target="ftn4"/> sind zum Beispiel folgende Unterschiede denkbar: Man könnte das Phänomen „Krankheit einer literarischen Figur“ ausschließlich daran festmachen, ob diese ärztlich behandelt wird. Man kann aber ebenso eine Reihe von Phänomenen wie körperliche Reaktionen, Aussagen der Figur etc. nutzen, um Krankheit zu bestimmen.
                    </p>
</div>
<div rend="DH-Heading2" type="div2">
<head>Kontextualisierung von Phänomenen</head>
<p style="text-align:left; ">Neben der Bestimmung der Teile, aus denen eine Phänomenbeschreibung zusammengesetzt ist, geht es auch um die Frage, welches Wissen zur Bestimmung des Phänomens herangezogen werden muss. Dies kann zum einen Wissen sein, das der Text vermittelt. Aber es kann auch weiteres Wissen nötig werden, wie etwa spezielles Domänenwissen, zusätzliches (innerfiktionales oder außerfiktionales) Weltwissen u.ä. Die Kernfrage ist entsprechend: Braucht man über das Textwissen hinausgehendes weiteres Wissen, um ein Phänomen zu identifizieren?</p>
<p style="text-align:left; ">Auch hier gilt: Die Einstufung der Komplexität gilt für den betrachteten Anwendungsfall, andere Fälle haben ggf. für dieselben Phänomene andere Komplexitätsgrade. Im Projekt Gender und Krankheit wurde etwa mit Koreferenz-Auflösung experimentiert, die überwiegend auf Textphänomenen basiert. Das Krankheitskonzept wiederum wurde unter Rückgriff auf Wissen für zeitgenössische Krankheiten und Krankheitsbezeichnungen bearbeitet (etwa „Phthise“ als Bezeichnung für Tuberkulose).</p>
<p style="text-align:left; ">Abbildung 1 stellt beispielhaft die beiden Dimensionen der Komplexität einiger Phänomene dar, die im Projekt Gender und Krankheit eine Rolle spielen. </p>
<figure>
  <graphic height="9.762647222222222cm" n="1001" rend="inline" url="168_final-78d4f4954a4b26eb09d63504d625bd91.png" width="12.747925cm"/>
  <head> <hi rend="italic">Abbildung 1</hi>: Komplexitätsdimensionen für Phänomene</head>
</figure>
</div>
<div rend="DH-Heading2" type="div2">
<head>Textheterogenität</head>
<p style="text-align:left; ">Oft wird vorschnell angenommen, dass für die textorientierten Digital Humanities die nun wesentlich größere Menge an untersuchten Texten distinktiv ist. Dabei ist die Frage, ob es sich um – vermeintliche – Big Data handelt oder nicht, aus Sicht der computationellen Textanalyse nur insofern interessant, als damit die Frage zusammenhängt, ob man die Texte, die man analysiert, kennt bzw. kennen kann oder nicht. In Bezug auf die Komplexität der genutzten Texte relevanter ist hingegen die umfassendere Frage: Wie viele (wie) verschiedene Texte werden analysiert? Dabei fällt unter Heterogenität von Texten die Anzahl der Texte selbst, aber auch die Anzahl von verschiedenen Texteigenschaften, die für die Fragestellung relevant sind bzw. sein könnten. Im Fall literarischer Texte sind das typischerweise Eigenschaften wie Gattung, Genre, Epoche, Autorgender, Erscheinungsort etc.</p>
<p style="text-align:left; ">Die Textheterogenität reicht von einem Text bis zu sehr vielen, sehr heterogenen Texten reicht<ref n="5" target="ftn5"/> und ist v.a. im Vergleich zu anderen Vorhaben beurteilbar. Im Projekt Gender und Krankheit liegt eine vergleichsweise hohe Textheterogenität vor, da das Korpus aus über 2.000 deutschsprachigen Texten besteht, die verschiedene Genres, Autor*innen und Epochen zuzuordnen sind.
                    </p>
<figure>
  <graphic height="9.600408333333334cm" n="1002" rend="inline" url="168_final-e4d46ca2fe9702aa4817a0ad9a37835d.png" width="8.171133333333334cm"/>
  <head><hi rend="italic">Abbildung 2</hi>: Komplexitätsdimension Textheterogenität </head>
</figure>
</div>
<div rend="DH-Heading2" type="div2">
<head>Analysemodus</head>
<p style="text-align:left; ">In der Komplexitätsdimension des Analysemodus geht es darum, wer die Erkenntnisse produziert. Hier sind die beiden Möglichkeiten recht offensichtlich: Auf der einen Seite steht (menschliches) Lesen, auf der anderen Seite maschinelles Erschließen. Die Hauptfrage ist also: Wird die Textbasis durch Menschen oder durch Computer erschlossen? Dabei wird für alle Zugänge als gegeben vorausgesetzt, dass der Computer genutzt wird. Während das Lesen in Annotationen von Textstellen oder zumindest in die Ergänzung der Texte um Metainformationen resultiert, wird beim maschinellen Erschließen im Normalfall Textmining betrieben. Beide Textzugangsarten können weiter differenziert werden nach der Interpretationstheorie (etwa in text-, leser- oder autororientierte Zugänge) bzw. dem angewendeten maschinellen Verfahren (etwa in regelbasierte und Lernverfahren).</p>
<p style="text-align:left; ">In konkreten Forschungsprojekten kommen fast immer beide Modi vor. So werden im Projekt Gender und Krankheit manuelle Annotationen von Textpassagen und halb-automatische Verfahren zur Wortfeldgenerierung für die weitere Verarbeitung oder die Methodenentwicklung mit automatischen Verfahren zur Figurenerkennung, Segmentierung und Sentimentanalyse kombiniert. Da die Zwischenschritte in der Analyse zumeist manuell überprüft und teilweise ergänzt werden, handelt es sich hier um ein Verfahren zwischen Lesen und automatischem Erschließen und damit um eine eher geringe Komplexität.</p>
</div>
<div rend="DH-Heading2" type="div2">
<head>Erkenntnisbeitrag</head>
<p style="text-align:left; ">Schließlich geht es bei der Betrachtung von computationellen Textanalysen auch darum, wie der Computer eingesetzt wird, um Erkenntnisse zu generieren. Wenn man von der literaturwissenschaftlichen Praxis der Textanalyse ausgeht, ist die komplexeste Aufgabe jene, die Textbasis insgesamt im Hinblick auf die gewählte Fragestellung zu interpretieren. Interpretation ist jedoch bislang nicht der Fokus computationeller Zugänge zu literarischen Texten. Trotzdem lohnt es sich, Interpretation als ein Extrem der Dimension der Erkenntnis zu denken. In Anlehnung an die literaturwissenschaftliche Praxis kann man die Komplexitätsdimension des Erkenntnisbeitrags computationeller Analysen als von der Analyse des Textes für ein erstes Textverständnis bis hin zur Interpretation der Textbasis als Ganzes ausgedehnt sehen.<ref n="6" target="ftn6"/> Alternativ kann auch die sozialwissenschaftliche Kategorisierung von Forschungslogiken<ref n="7" target="ftn7"/> in Anlehnung an Peirce (1935) in Deduktion, Induktion und Abduktion als Skala für die Erkenntnisdimension genutzt werden.
</p>
<p style="text-align:left; ">Unabhängig von der Frage, welche Systematik man für die Tätigkeiten verwendet, die mit Textverstehen befasst sind, ist die zentrale Frage in der letzten Komplexitätsdimension: Wie weit geht der Erkenntnisbeitrag der computationellen Methode? Es geht also um die Frage nach der Neuheit des computationell Erforschten. Grob kann man die Komplexitätsstufen des Erkenntnisbeitrags wie folgt erfassen: Werden in einer deduktiven bzw. einfachen Textanalyse aufgrund von bestehenden Hypothesen bzw. Regeln (also bestehenden Analysekategorien und -verfahren) durchgeführt, werden aus der Betrachtung von Texten neue Analysekategorien oder auch Taxonomien entwickelt oder handelt es sich um Hypothesen über größere Zusammenhänge in den Texten, also um ihre Interpretation?<ref n="8" target="ftn8"/>
</p>
<p style="text-align:left; ">Bei der Auseinandersetzung mit der Komplexitätsdimension des Erkenntnisbeitrags ist zu beachten, dass in einer typischen literaturwissenschaftlichen Textanalyse meist alle Modi vorliegen und fließend ineinander übergehen. Für die Komplexitätseinschätzung ist relevant, welche Modi davon computationell unterstützt werden sollen. Im Fall des Projekts zu Gender und Krankheit soll etwa deduktiv die Veränderung der Figurenkonstellation anhand der Figurennennungen analysiert werden. Ein induktives Verfahren liegt vor, wenn Genderkategorien durch Clustering von Figurenrede herausgearbeitet werden (die dann wieder deduktiv in der Analyse genutzt werden). Und schließlich liegt ein abduktiver Zugang vor, wenn durch eine Gesamtbetrachtung ein neues Element entdeckt würde, das Figurenkrankheit beeinflusst.</p>
<figure>
  <graphic height="10.032808333333334cm" n="1003" rend="inline" url="168_final-2a6a6efd9fb89797056502231133483f.png" width="12.58018888888889cm"/>
  <head><hi rend="italic">Abbildung 3</hi>: Komplexitätsdimensionen für Erkenntnis: Analysemodus und Erkenntnisbeitrag </head>
</figure>
</div>
</div>
<div rend="DH-Heading1" type="div1">
<head>Zur Nutzung des Modells</head>
<p style="text-align:left; ">Wie bereits dargelegt, betrifft die Bestimmung der Komplexität in den fünf Dimensionen primär die normativen Setzungen durch die Forscher*innen. Ausschlaggebend ist weniger, wie Texte, Phänomene und Erkenntnis an sich modelliert werden 
                    <hi rend="italic">sollten</hi>, sondern vielmehr, wie die Modellierung konkret umgesetzt wird. Die vorgeschlagenen Dimensionen sind außerdem von der mit einem Zugang verbundenen Interpretationstheorie unabhängig. Damit ist das Modell für alle literaturwissenschaftlichen Textanalyseverfahren geeignet, für jene, die in einer strukturalistischen Tradition gesehen werden können, genauso wie für solche, die eher postmoderne Zugangsweisen umsetzen – oder andere Zugänge.
                </p>
<p style="text-align:left; ">Für die Betrachtung und Kritik eines Zugangs sollten alle fünf Dimensionen berücksichtigt werden. Damit vermeidet man auch vorschnelle Kritik, die sich auf eine einfache Modellierung einer Dimension beschränkt und den Zugang insgesamt als unterkomplex betrachtet, obwohl er in einer oder mehreren anderen Dimensionen Erhebliches leistet. </p>
<p style="text-align:left; ">Darüber hinaus eignet sich das Modell als Instrument für den Entwurf eines Zugangs. Es kann in allen Phasen computationeller Textanalyse genutzt werden – vom Design des Forschungszugangs zu Beginn der Forschungsarbeit über die wiederholten Bestandsaufnahme oder Nachjustierung im Projektverlauf bis hin zur Einordnung der erzielten Ergebnisse am Ende und der Reflektion des gesamten Prozesses.</p>
<figure>
  <graphic height="9.005463888888888cm" n="1004" rend="inline" url="168_final-962c443b0da6c708e2ad4b5c6d0c060e.png" width="14.538719444444444cm"/>
  <head> <hi rend="italic">Abbildung 4</hi>: Übersichtsdarstellung: Komplexität im Projekt Gender und Krankheit</head>
</figure>
<p style="text-align:left; ">Abschließend seien noch einmal die fünf Dimensionen mit ihren Kernfragen dargestellt:</p>
<list type="ordered">
<item>
  Komplexitätsdimension 1: die Zusammengesetztheit von Phänomenen
  <list type="unordered">
    <item>Frage
    <hi rend="italic">: Wird das Phänomen als einfach, nicht weiter unterteilt, oder als aus mehreren Phänomenen zusammengesetzt betrachtet?</hi>
    </item>
    <item>Komplexität: von einfach bis hin zu vielfach zusammengesetzten Phänomenen</item>
  </list>
</item>
<item>Komplexitätsdimension 2: die Kontextualisierung von Phänomenen
<list type="unordered">
  <item>Frage: 
  <hi rend="italic" xml:space="preserve">Braucht man über das Textwissen hinausgehendes weiteres Wissen, um ein Phänomen zu identifizieren? </hi>
  </item>
  <item>Komplexität: von Textwissen bis hin zu verschiedenen Arten von umfangreichem weiterem Wissen</item>
</list>
</item>
<item>Komplexitätsdimension 3: Textheterogenität
<list type="unordered">
  <item>Frage
  <hi rend="italic">: Wie viele (wie) verschiedene Texte werden analysiert?</hi>
  </item>
  <item>Komplexität: von einem Text mit homogenen Eigenschaften bis hin zu vielen, in sich und zueinander heterogenen Texten </item>
</list>
</item>
<item>Komplexitätsdimension 4: Analysemodus
<list type="unordered">
  <item>Frage
  <hi rend="italic">: Wird die Textbasis durch Menschen oder durch Computer erschlossen</hi>? 
  </item>
  <item>Komplexität: von von Menschen annotiert bis hin zu von Maschinen durch Lernen analysiert</item>
</list>
</item>
<item>Komplexitätsdimension 5: der Erkenntnisbeitrag computationeller Analysen
<list type="unordered">
  <item>Frage: 
  <hi rend="italic">Wie weit geht der Erkenntnisbeitrag der computationellen Methode?</hi>
</item>
<item>Komplexität: von der Anwendung simpler Regeln auf einzelne Textelemente bis zur Interpretation der gesamten Textbasis.</item>
</list>
</item>
</list>
</div>
</body>
<back>
  <div type="notes">
    <note n="1" rend="footnote text" xml:id="ftn1">
      Vgl. dazu den Artikel „The Digital Humanities Debacle. Computational methods repeatedly come up short“ von Da (2019b) und als „Digital Humanities War“ zusammengefasste Reaktionen auf 
      <ref target="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986</ref>, sowie das „Special Forum on Responses to Nan Z. Da“ in 
      <hi rend="italic">Cultural Analytics</hi> auf 
      <ref target="https://culturalanalytics.org/2019/09/special-forum-on-responses-to-nan-z-da/">https://culturalanalytics.org/2019/09/special-forum-on-responses-to-nan-z-da/</ref>, sowie Jannidis (2019) und Krajewski (2019).
    </note>
    <note n="2" rend="footnote text" xml:id="ftn2">
      Nan Z. Da (2019b) könnte diesbezüglich so zusammengefasst werden, dass sie vorschlägt, keine Mittel mehr in die Computational Literary Studies zu stecken und damit zu verschwenden. 
    </note>
    <note n="3" rend="footnote text" xml:id="ftn3">
      Damit ist der Zugang wesentlich spezifischer als die umfassende „TaDiRAH - Taxonomy of Digital Research Activities in the Humanities“, die alle digitalen Forschungsaktivitäten zu erfassen versucht (vgl. 
      <ref target="http://tadirah.dariah.eu/vocab/">http://tadirah.dariah.eu/vocab/</ref>, gesehen am 21.12.2019).
    </note>
    <note n="4" rend="footnote text" xml:id="ftn4">
      Vgl. dazu z.B. Gius et al. (2019), Andresen et al. (2019) und 
      <ref target="https://www.herma.uni-hamburg.de/subprojects.html">https://www.herma.uni-hamburg.de/subprojects.html</ref>
      <hi rend="Hyperlink" xml:space="preserve">, </hi>gesehen am 21.12.2019). 
    </note>
    <note n="5" rend="footnote text" xml:id="ftn5">
      Genau genommen werden hier zwei Gegensatzpaare abgebildet: Anzahl (von Texten) und Heterogenität (von Texteigenschaften). Diese Eigenschaften werden zu einer Dimension zusammengefasst, da sie die Komplexität von Texten vergleichbar steigern.
    </note>
    <note n="6" rend="footnote text" xml:id="ftn6">
      „Textanalyse“ ist literaturwissenschaftlich mehrdeutig, da der Begriff sowohl eine Textanalyse meint, die das Textverständnis im Fokus hat und der anschließenden Interpretation als Voraussetzung dient, als auch den Prozess der Analyse und Interpretation insgesamt, vgl. dazu Winko (2003).  
    </note>
    <note n="7" rend="footnote text" xml:id="ftn7">
      Vgl. dazu auch die Arbeit im Projekt hermA zu den verschiedenen Forschungslogiken im Kontext von Annotationen (Gaidys et al. 2017 bzw. www.herma.uni-hamburg.de).
    </note>
    <note n="8" rend="footnote text" xml:id="ftn8">
      Vgl. dazu auch Eco (1987): „[D]er Text ist ein Objekt, das die Interpretation im Verlauf ihrer zirkulären Anstrengungen um die eigene Schlüssigkeit bildet auf der Basis dessen, was sie als ihr Resultat erschafft. Ich schäme mich nicht, daß ich auf diese Weise den alten und immer noch gültigen hermeneutischen Zirkel definiere. Die Logik der Interpretation ist die Peircesche Logik der ,Abduktion‘.“
    </note>
  </div>
<div type="bibliogr">
<listBibl>
<head>Bibliographie</head>
<bibl style="text-align:left; ">
  <hi rend="bold">Adelmann, Benedikt / Melanie Andresen / Anke Begerow / Lina Franken / Evelyn Gius /  Michael Vauth</hi> (2019): „Evaluation of a Semantic Field-Based Approach to Identifying Text Sections about Specific Topics“. In 
  <hi rend="italic">DH2019 Book of Abstracts</hi>. Utrecht.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Bode, Katherine</hi>. Im Erscheinen. „Why you can’t model away bias“. Preprint: Modern Language Quarterly 80.3.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Da, Nan Z.</hi> (2019a): „The Computational Case against Computational Literary Studies“. 
  <hi rend="italic">Critical Inquiry</hi> 45 (3): 601–39. 
  <ref target="https://doi.org/10.1086/702594">https://doi.org/10.1086/702594</ref>.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Da, Nan Z.</hi> (2019b): „The Digital Humanities Debacle“. 
  <hi rend="italic">The Chronicle of Higher Education</hi>, 27. März 2019. 
  <ref target="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986</ref>.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Dobson, James E.</hi> (2019): 
  <hi rend="italic">Critical Digital Humanities: The Search for a Methodology</hi>. Topics in the digital humanities. Urbana, Illinois: University of Illinois Press.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Eco, Umberto</hi> (1987): 
  <hi rend="italic">Lector in Fabula. Die Mitarbeit der Interpretation in erzählenden Texten</hi>. München: Hanser.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Gaidys, Uta / Evelyn Gius / Margarete Jarchow / Gertraud Koch / Wolfgang Menzel / Dominik Orth /  Heike Zinsmeister</hi>
  (2017) : „Project description – hermA: Automated modelling of hermeneutic processes“. 
  <hi rend="italic">Hamburger Journal für Kulturanthropologie</hi>. 
  <ref target="https://journals.sub.uni-hamburg.de/hjk/article/view/1213">https://journals.sub.uni-hamburg.de/hjk/article/view/1213</ref>.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Gius, Evelyn / Katharina Krüger /  Carla Sökefeld</hi> (2019): „Korpuserstellung als literaturwissenschaftliche Aufgabe“. In 
  <hi rend="italic">DHd 2019 Digital Humanities: multimedial &amp; multimodal Konferenzabstracts</hi>, 164–166. Frankfurt &amp; Mainz.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold" xml:space="preserve">Jannidis, Fotis. </hi> (2019): „Digitale Geisteswissenschaften: Offene Fragen - schöne Aussichten“. Herausgegeben von Lorenz Engell und Bernhard Siegert. 
  <hi rend="italic">Zeitschrift für Medien- und Kulturforschung</hi>. 
  <ref target="https://doi.org/DOI:%2010.28937/ZMK-10-1">https://doi.org/DOI: 10.28937/ZMK-10-1</ref>.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Krajewski, Markus</hi> (2019):  „Hilfe für die digitale Hilfswissenschaft. Eine Positionsbestimmung“. Herausgegeben von Lorenz Engell und Bernhard Siegert. 
  <hi rend="italic">Zeitschrift für Medien- und Kulturforschung</hi>. 
  <ref target="https://doi.org/DOI:%2010.28937/ZMK-10-1">https://doi.org/DOI: 10.28937/ZMK-10-1</ref>.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Peirce, Charles S</hi> (1935): 
  <hi rend="italic">Collected Papers of Charles Sanders Peirce, Volumes V and VI: Pragmatism and Pragmaticism and Scientific Metaphysics</hi>. Herausgegeben von Charles Hartshorne und Paul Weiss. Cambridge, Mass: Belknap Press of Harvard Univ. Press.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Winko, Simone</hi> (2003): „Textanalyse“. In 
  <hi rend="italic">Reallexikon der deutschen Literaturwissenschaft: Neubearbeitung des Reallexikons der deutschen Literaturgeschichte</hi>, herausgegeben von Harald Fricke, Klaus Grubmüller, Jan-Dirk Müller, und Klaus Weimar, 3., neubearb. Aufl. Berlin: De Gruyter:  597–601.
</bibl>
<bibl style="text-align:left; ">
  <hi rend="bold">Versch. Autoren</hi> (2019): „Special Forum on Responses to Nan Z. Da“. Journal of Cultural Analytics. 17. September 2019. 
  <ref target="https://culturalanalytics.org/2019/09/special-forum-on-responses-to-nan-z-da/">https://culturalanalytics.org/2019/09/special-forum-on-responses-to-nan-z-da/</ref>.
</bibl>
</listBibl>
</div>
</back>
</text>
</TEI>
