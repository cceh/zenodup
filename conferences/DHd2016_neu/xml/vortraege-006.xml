<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="vortraege-006">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>Automatische Textanalysen in der Geschichtswissenschaft – Auswertung, Interpretation und Relevanz </title>
        <author>
          <name>
            <surname>Fiedler</surname>
            <forename>Maik</forename>
          </name>
          <affiliation>Georg Eckert Institut für internationale Schulbuchforschung, Deutschland</affiliation>
          <email>fiedler@gei.de</email>
        </author>
        <author>
          <name>
            <surname>Weiß</surname>
            <forename>Andreas</forename>
          </name>
          <affiliation>Georg Eckert Institut für internationale Schulbuchforschung, Deutschland</affiliation>
          <email>weiss@gei.de</email>
        </author>
        <author>
          <name>
            <surname>Heuwing</surname>
            <forename>Ben</forename>
          </name>
          <affiliation>Institut für Informationswissenschaft &amp; Sprachtechnologie, Universität Hildesheim</affiliation>
          <email>heuwing@uni-hildesheim.de</email>
        </author>
        <author>
          <name>
            <surname>Schnober</surname>
            <forename>Carsten</forename>
          </name>
          <affiliation>Ubiquitous Knowledge Processing Lab, Deutsches Institut für Internationale Pädagogische Forschung / Technische Universität Darmstadt</affiliation>
          <email>schnober@ukp.informatik.tu-darmstadt.de</email>
        </author>
      </titleStmt>
      <editionStmt>
        <edition>
          <date>2015-10-15T15:22:00Z</date>
        </edition>
      </editionStmt>
      <publicationStmt>
        <publisher>Elisabeth Burr, Universität Leipzig</publisher>
        <address>
          <addrLine>Beethovenstr. 15</addrLine>
          <addrLine>04107 Leipzig</addrLine>
          <addrLine>Deutschland</addrLine>
          <addrLine>Elisabeth Burr</addrLine>
        </address>
      </publicationStmt>
      <sourceDesc>
        <p>Converted from a Word document </p>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <appInfo>
        <application ident="DHCONVALIDATOR" version="1.14">
          <label>DHConvalidator</label>
        </application>
      </appInfo>
    </encodingDesc>
    <profileDesc>
      <textClass>
        <keywords scheme="ConfTool" n="category">
          <term>Vortrag</term>
        </keywords>
        <keywords scheme="ConfTool" n="subcategory">
          <term></term>
        </keywords>
        <keywords scheme="ConfTool" n="keywords">
          <term>Meta-Analyse</term>
          <term>Validierung</term>
          <term>Topic Modelling</term>
        </keywords>
        <keywords scheme="ConfTool" n="topics">
          <term>Teilen</term>
          <term>Umwandlung</term>
          <term>Datenerkennung</term>
          <term>Inhaltsanalyse</term>
          <term>Übersetzung</term>
          <term>Modellierung</term>
          <term>Kommunikation</term>
          <term>Theoretisierung</term>
          <term>Bereinigung</term>
          <term>Bewertung</term>
          <term>Webentwicklung</term>
          <term>Visualisierung</term>
          <term>Daten</term>
          <term>Infrastruktur</term>
          <term>Metadaten</term>
          <term>Methoden</term>
          <term>Forschungsprozess</term>
          <term>Text</term>
        </keywords>
      </textClass>
    </profileDesc>
  </teiHeader>
  <text>
    <body>
      <div type="div1" rend="DH-Heading">
        <head>Motivation und Fragestellung</head>
        <p>In jedem Digital-Humanities-Projekt (DH) stellt sich die Frage von neuem: Welche
          „Relevanz haben die Modellierung, Vernetzung und Visualisierung für die
          Geistesartefakte selbst und für den Gewinn reproduzierbarer wissenschaftlicher
          Erkenntnisse über sie? Im Projekt „Welt der Kinder“ (WdK) wurden diese Punkte
          mit Hilfe von Topic Modeling und Text-Mining-Werkzeugen mit in der
          Geschichtswissenschaft anerkannten Thesen in einem kontrollierten Verfahren
          überprüft. Es handelt sich bei WdK mit seinem repräsentativen Textkorpus von
          über 3000 historischen Schulbüchern um ein bisher weltweit einzigartiges
          Projekt, das für künftige ähnliche Vorhaben vorbildhaft sein will. </p>
          <p>Aus Sicht der klassischen Geschichtswissenschaften gibt es bei der Bearbeitung
            großer Datenmengen häufig Argwohn gegenüber der Sekundäranalyse maschinell
            generierter Ergebnisse, verstärkt durch mangelndes Wissen über fachfremde
            Methodik. Dies lässt die Ergebnisse der DH oft als zweifelhaft oder nicht
            neuwertig erscheinen. Zusätzlich können Verzerrungen durch die Zusammensetzung
            einer Textsammlung entstehen, durch die Dokumentenauswahl und des zu
            analysierenden Vokabulars sowie aus den darauf aufbauenden Aggregierungen und
            Visualisierungen (Chuang et al. 2012). Große Datenmengen erfordern ein anderes
            Vorgehen bei der Auswertung als die traditionell in den Geschichtswissenschaften
            üblichen Verfahren. Die Methode der automatischen Textanalyse stellen trotzdem
            eine durch die Forschungsziele beeinflusste subjektive Sichtweise auf die
            vorhandenen Daten dar (DiMaggio et al. 2013). Wir zeigen an Hand eines in WdK
            vorgenommen Validierungsexperiments, welche Aushandlungsprozesse notwendig
            waren, um nachnutzbare und nachvollziehbare Ergebnisse zu erhalten. </p>
            <p>Für die Meta-Analyse von klassischen und digitalen geschichtswissenschaftlichen
              Herangehensweisen ist die Beantwortung folgender Fragen prioritär:</p>
              <p>
                <hi rend="bold">Erstens)</hi> Wie können auf klassischem Weg erbrachte
                Ergebnisse für die DH so codifiziert werden, dass sie nicht nur für Menschen
                interpretierbar, sondern auch durch die digitalen Werkzeuge reproduzierbar sind?
                Sinn dieses Verfahrens ist es, Versuchsanordnungen und Analysen so aufzubauen,
                dass diese nicht immer „bei Null“ beginnen müssen, sondern, wie ein klassischer
                Fachtext, anerkannte Annahmen und Erkenntnisse implizit transportieren und
                wiederholen. </p>
                <p>
                  <hi rend="bold">Zweitens)</hi> Wie kann die Belastbarkeit von Ergebnissen, die
                  mit Hilfe von Methoden der automatischen Textmodellierung auf einem
                  umfangreichen Korpus erbracht worden sind, validiert werden? </p>
                  <p>
                    <hi rend="bold">Drittens)</hi> Wie kann man die Leistung digitaler Methoden für
                    explorative Analysen anwenden, ohne auf ein bereits feststehendes Ziel
                    hinzuarbeiten? </p>
                    <p>
                      <hi rend="bold">Viertens)</hi> Wie müssen die Versuchsanordnung und das Projekt
                      aufgebaut werden, um den Daten zu vertrauen und sie interpretieren sowie
                      kontextualisieren zu können? </p>
                      <p>Der Vortrag wird den Arbeitsprozess (interdisziplinäre Arbeit an historischen
                        Thesen mit Hilfe digitaler Tools) analysieren, die verschiedenen
                        fachspezifischen Methoden problematisieren sowie schlaglichtartig Wege
                        beleuchten, die zu möglichen Antworten auf die gestellten Fragen führen können.
                      </p>
                    </div>
                    <div type="div1" rend="DH-Heading">
                      <head>Werkzeuge</head>
                      <p>Die Grundlage der Topic-Modelling-basierten Analyse besteht auf im Bereich DH
                        etablierter Methoden wie LDA (<hi rend="italic">Latent Dirichlet
                        Allocation</hi>; Blei et al. 2003). Dieses Verfahren ordnet Begriffe auf Basis
                        von Kookkurrenz und statistischen Analysen einander zu und extrahiert Topics in
                        Form gewichteter Wortlisten. Diese ergeben für menschliche Benutzer
                        interpretierbare Listen, und erlauben eine automatische Inferenz von
                        Topic-Verteilungen innerhalb eines Dokuments.</p>
                        <p>Die Validierungsstudie wurde mit einem interaktiven Prototyp durchgeführt, der
                          die Texte im Korpus und Statistiken über die Ergebnismengen zugänglich macht.
                          Suchanfragen können sich auf Metadaten – beispielsweise Jahr und Ort der
                          Veröffentlichung oder Schultyp – Termanfragen und Topic-Verteilungen beziehen.
                          Ergebnisse werden mit Statistiken zur Topic-Intensität und relativen
                          Dokumentenhäufigkeit im Zeitverlauf ausgegeben. </p>
                        </div>
                        <div type="div1" rend="DH-Heading">
                          <head>Vorgehen bei der Validierung:</head>
                          <p>Belastbarkeitsüberprüfungen bauen Vertrauen in datenbasierte, historische
                            Schlussfolgerungen und Annahmen auf. So wird überprüft, ob die statistischen
                            Modelle existierende Erkenntnisse mehrheitlich bestätigen, und als wie
                            zuverlässig bestätigende oder widerlegende Ergebnisse eingeschätzt werden
                            (DiMaggio et al. 2013; Evans 2014). Die im Experiment bearbeiteten historischen
                            Thesen stellten Sachverhalte dar, die sich quantitativ überprüfen lassen, etwa
                            durch den Vergleich von Topic-Verteilungen (Newman / Block 2006; Yang et al.
                            2011), und im Nachhinein von Experten für das jeweilige Fachgebiet in Hinblick
                            auf ihre Plausibilität überprüft werden. </p>
                            <p>Für die Validierungsstudie wurden zu überprüfende Thesen vorab definiert, um
                              Abweichungen von der ursprünglichen Fragestellung zu dokumentieren. Sie sind
                              repräsentativ für reale historische Fragestellungen im Rahmen des Projektes
                              (Kolonien und Auswanderung; Französische Revolution und Befreiungskriege;
                              deutsche Kriegsflotte). Dabei wurden in einem ersten Schritt Begrifflichkeiten
                              und Interpretationen der Fragestellungen in interdisziplinären Arbeitsgruppen
                              diskutiert, um fachliche Verständnisschwierigkeiten auszuräumen. Da die Thesen
                              erschöpfend und präzise mit den vorhandenen Werkzeugen untersucht wurden, bilden
                              auch die Auswertungsstrategien mögliche Vorgehensweisen für die Überprüfung
                              bereits vorliegender Hypothesen ab.</p>
                            </div>
                            <div type="div1" rend="DH-Heading">
                              <head>Auswertung</head>
                              <p>Bei der Analyse der Thesen zeigten sich unterschiedliche Strategien für die
                                einzelnen Schritte der Auswertung. Wichtig hierbei war, ob unterschiedliche
                                Herangehensweisen, vergleichbare Ergebnisse reproduzierten. Die Ergebnisse der
                                einzelnen Arbeitsgruppen widersprachen einander an wenigen Stellen, und
                                gegebenenfalls primär in ihrer Bewertung der Verlässlichkeit der Ergebnisse. Die
                                vorgegebenen geschichtswissenschaftlichen Thesen wurden in den Versuchen mit
                                Topic-Modellen größtenteils bestätigt und zusätzlich mittels Termanfragen
                                validiert.</p>
                                <p>Das Vorgehen bei den Topic-Modelling-basierten Analysen beinhaltete im ersten
                                  Schritt eine Suche nach relevanten Topics an Hand einzelner Terme. Dabei zeigte
                                  sich, dass die Topics in Modellen mit einer manuell überschaubaren Topic-Anzahl
                                  (50, 100, 200) für spezielle historische Forschungsfragen zu allgemein oder auch
                                  zu spezifisch ausfielen. Teilweise wurden daraufhin die Thesen stellvertretend
                                  an Hand thematischer Teilgebiete oder übergeordneter Themen untersucht.</p>
                                  <p>Für eine höhere Genauigkeit wurden auch Kombinationen aus Termsuche und
                                    Dokumentenfiltern auf Basis automatisch generierter Topics eingesetzt. Für eine
                                    Bewertung der Abfragegenauigkeit wurden manuelle Inspektionen der relevantesten
                                    Trefferdokumente durchgeführt und Anfragen iterativ neu formuliert. Um für die
                                    Validierung eine Vergleichsebene bereitzustellen, wurden zusätzliche Analysen
                                    nur auf der Grundlage manuell und mittels historischen Vorwissens gewählter
                                    Terme durchgeführt.</p>
                                  </div>
                                  <div type="div1" rend="DH-Heading">
                                    <head>Schlussfolgerungen</head>
                                    <p>
                                      <anchor xml:id="move43120845513"/>Zusammengefasst kann zwischen zwei
                                      grundlegenden Vorgehensweisen unterschieden werden. In der ersten Variante
                                      werden die aufgestellten Thesen konfirmatorisch überprüft. Diese werden dafür
                                      formalisiert und in Form von Suchanfragen und zu erwartenden Ergebnissen
                                      operationalisiert. Die Ergebnisse werden dann vor allem hinsichtlich der
                                      erwarteten Zeitverläufe und relativen Unterschiede zwischen Untermengen
                                      interpretiert. </p>
                                      <p>Die explorative Herangehensweise an die Datenanalyse berücksichtigt dagegen auch
                                        andere Hinweise aus den Ergebnissen, und sucht nach Erklärungen für beobachtete
                                        Auffälligkeiten. Die Aussagekraft der Ergebnisse kann dabei jedoch dadurch
                                        eingeschränkt werden, dass die untersuchten Thesen erst mit Kenntnis der Daten
                                        formuliert worden sind. Eine Strategie, um diese Unsicherheit auszugleichen,
                                        besteht darin, Evidenz für eine Aussage mit mehreren unterschiedlichen
                                        Vorgehensweisen zu sammeln.</p>
                                        <p>
                                          <anchor xml:id="move431207206"/>
                                          <anchor xml:id="move43120720614"/>Diese Ergebnisse zeigen den potentiellen
                                          Mehrwert von DH an, da mit Hilfe computerlinguistischer und
                                          informationswissenschaftlicher Methoden klassische Thesen aus der
                                          Geschichtswissenschaft präzisiert werden konnten. Die Interpretation
                                          quantitativer Ergebnisse, etwa als Diagramm visualisiert, konnte sich nach
                                          Bedarf auf die vorab definierten Vorannahmen beschränken. Die Einbeziehung
                                          größerer zeitlicher Kontexte erforderte teilweise, die dargestellten Verläufe
                                          und Tendenzen mit verschiedenen Zeitspannen neu zu interpretieren. Als wichtige
                                          Vorgehensweise hat sich hier die Bildung eines gleitenden Durchschnitts über
                                          längere Zeiträume erwiesen, um thematische Tendenzen zuverlässiger
                                          interpretieren zu können. </p>
                                          <p>Als wichtiger Faktor stellte sich auch die Qualität der OCR-Digitalisierung
                                            heraus. Bei Daten aus historischen Quellen (Schriftbild Sütterlin / Fraktur)
                                            werden auch mit aktueller Technologie aufgrund der verwendeten Schriftarten
                                            teilweise über 10 Prozent der Zeichen falsch erkannt, was bei der Auswertung der
                                            maschinell generierten Topics durch die Benutzer zu Problemen bei der
                                            Interpretation und Weiterverwendung führt. Daher muss die Frage gestellt werden,
                                            wie Daten zukünftig in den Vorverarbeitungsschritten aufbereitet werden, damit
                                            Topic Modelling und andere automatische Methoden zu hilfreichen und
                                            interpretierbaren Ergebnissen führen.</p>
                                            <p>Neben der Einbeziehung von Topic Models, die auf unterschiedliche Perspektiven
                                              optimiert wurden, werden im Rahmen des Projektes andere Herangehensweisen an die
                                              statistische Textmodellierung, wie z. B. Clustering-Verfahren, in Hinblick auf
                                              ihre Anwendbarkeit und Robustheit vergleichend evaluiert. In diesem Zusammenhang
                                              ist es wichtig, thematisch relevante Topics einfach auffindbar zu machen und sie
                                              für Anfragen kombinieren zu können. Des Weiteren sollten Topics geordnet nach
                                              Themen oder Diskursfeldern und / oder -strängen präsentiert werden sowie in
                                              einer leicht lesbaren Anzeige deren synchrone und diachrone Verteilungen
                                              herausstellen, wobei Ungleichverteilungen innerhalb der Untersuchungsmenge und
                                              die Zuverlässigkeit statistischer Aggregierungen deutlich gemacht werden müssen. </p>
                                            </div>
                                          </body>
                                          <back>
                                            <div type="bibliogr">
                                              <listBibl>
                                                <head>Bibliographie</head>
                                                <bibl>
                                                  <hi rend="bold">Blei, David. M. / Ng, Andrew. Y. / Jordan, Michael I.</hi>
                                                  (2003): "Latent Dirichlet allocation", in: <hi rend="italic">Journal of
                                                  Machine Learning Research</hi> 3: 993–1022.</bibl>
                                                  <bibl>
                                                    <hi rend="bold">Chuang, Jason / Ramage, Daniel / Manning, Chistopher / Heer,
                                                      Jeffrey </hi> (2012): "Interpretation and Trust: Designing Model-driven
                                                      Visualizations for Text Analysis", in: <hi rend="italic">Proceedings of the
                                                      SIGCHI Conference on Human Factors in Computing Systems</hi>, <hi
                                                      rend="italic">CHI ’12</hi>. New York, NY, USA: ACM 443–452. </bibl>
                                                      <bibl>
                                                        <hi rend="bold">DiMaggio, Paul / Nag, Manish / Blei, David</hi> (2013):
                                                        "Exploiting affinities between topic modeling and the sociological
                                                        perspective on culture: Application to newspaper coverage of U.S. government
                                                        arts funding", in: <hi rend="italic">Poetics</hi> 41, 6: 570–606. </bibl>
                                                        <bibl>
                                                          <hi rend="bold">Evans, Michael S.</hi> (2014): "A Computational Approach to
                                                          Qualitative Analysis in Large Textual Datasets", in: <hi rend="italic">PLoS
                                                          ONE</hi>9, 2, e87908. </bibl>
                                                          <bibl>
                                                            <hi rend="bold">Kaplan, Frédéric</hi> (2015): "A map for Big Data research
                                                            in Digital Humanities", in: <hi rend="italic">Frontiers in Digital
                                                            Humanities</hi> 2, 1: <ref
                                                            target="http://journal.frontiersin.org/article/10.3389/fdigh.2015.00001/abstract"
                                                            >http://journal.frontiersin.org/article/10.3389/fdigh.2015.00001/abstract</ref>
                                                            [letzter Zugriff 08. Januar 2016]. </bibl>
                                                            <bibl>
                                                              <hi rend="bold">Newman, David J. / Block, Sharon </hi> (2006):
                                                              "Probabilistic topic decomposition of an eighteenth-century American
                                                              newspaper", in: <hi rend="italic">Journal of the American Society for
                                                              Information Science and Technology</hi> 57, 6: 753–767. </bibl>
                                                              <bibl>
                                                                <hi rend="bold">Yang, Tze-I. / Torget, Andrew J. / Mihalcea, Rada </hi>
                                                                (2011): "Topic modeling on historical newspapers", in: <hi rend="italic"
                                                                >Proceedings of the 5th ACL-HLT Workshop on Language Technology for
                                                                Cultural Heritage, Social Sciences, and Humanities</hi>. Association for
                                                                Computational Linguistics 96–104. </bibl>
                                                              </listBibl>
                                                            </div>
                                                          </back>
                                                        </text>
                                                      </TEI>
